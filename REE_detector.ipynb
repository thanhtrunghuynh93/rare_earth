{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f05cd3f-eab1-49fd-80e0-6e20e3579cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n",
      "has_REE\n",
      "1    2026\n",
      "0    1088\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Removed REE-specific columns to prevent leakage: ['REE_Mins', 'REE', 'HREE_Note', 'LREE_Note', 'REE_Ratio', 'Sig_Mins']\n",
      "\n",
      "Filled missing values for key columns\n",
      "\n",
      "Columns with >50% missing values:\n",
      "Oth_Mins      0.598908\n",
      "Age_Mzn       0.748555\n",
      "Age_Ma        0.958253\n",
      "Host_Age      0.736994\n",
      "HAge_Ma       0.941875\n",
      "Host_Unit     0.885678\n",
      "Assoc_Rock    0.883109\n",
      "Alteration    0.925819\n",
      "Company       0.818240\n",
      "Comments      0.644188\n",
      "Discov_Yr     0.947977\n",
      "Expl_Note     0.990366\n",
      "Mine_Meth     0.969171\n",
      "PStat_Note    0.947013\n",
      "P_Years       0.984265\n",
      "P_refs        0.983622\n",
      "P_Note        0.956969\n",
      "RR_Ore_Mt     0.897559\n",
      "RR_TREO_Mt    0.937701\n",
      "RR_TREOgrd    0.927425\n",
      "RR_REE_grd    0.985870\n",
      "RR_Cutoff     0.988439\n",
      "RR_HM_Mt      0.987797\n",
      "RR_HM_pct     0.982659\n",
      "RR_min_Mt     0.990687\n",
      "RR_min_pct    0.999037\n",
      "RR_mon_Mt     0.955363\n",
      "RR_mon_pct    0.963070\n",
      "RR_oth_grd    0.979769\n",
      "RR_Yr_Est     0.927425\n",
      "RR_Refs       0.826911\n",
      "RR_RegCode    0.835902\n",
      "RR_Note       0.961143\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD DATA AND DEFINE TARGET\n",
    "# ============================================================================\n",
    "\n",
    "df = pd.read_csv(\"global.csv\")\n",
    "\n",
    "# Define target BEFORE dropping any columns\n",
    "# Use multiple indicators for robustness\n",
    "df[\"has_REE\"] = df[\"REE_Mins\"].notna().astype(int)\n",
    "\n",
    "print(f\"Target distribution:\\n{df['has_REE'].value_counts()}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: IDENTIFY AND REMOVE REE-SPECIFIC COLUMNS (PREVENT DATA LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "# These columns contain information that would only be known AFTER \n",
    "# determining REE presence - they cannot be used as features\n",
    "ree_specific_cols = [\n",
    "    'REE_Mins',      # Specific REE minerals present\n",
    "    'REE',           # REE grade/content information\n",
    "    'HREE_Note',     # Heavy REE notes\n",
    "    'LREE_Note',     # Light REE notes\n",
    "    'REE_Ratio',     # HREE/LREE ratio\n",
    "    'Sig_Mins',      # Often contains REE mineral names\n",
    "]\n",
    "\n",
    "# Remove these columns from the dataset\n",
    "existing_ree_cols = [col for col in ree_specific_cols if col in df.columns]\n",
    "df = df.drop(columns=existing_ree_cols)\n",
    "\n",
    "print(f\"Removed REE-specific columns to prevent leakage: {existing_ree_cols}\\n\")\n",
    "\n",
    "df['Rec_Type_clean'] = df['Rec_Type'].str.replace(r'\\(\\?\\)', '', regex=True).str.strip()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: HANDLE MISSING VALUES FOR KEY COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "# Fill nulls for columns we need for feature engineering\n",
    "key_columns_fillna = {\n",
    "    'Dep_Type': 'unknown',\n",
    "    'Dep_Form': 'unknown',\n",
    "    'Dep_Note': '',\n",
    "    'Rec_Note': '',\n",
    "    'Stat_Note': '',\n",
    "    'Commods': '',\n",
    "    'Components': '',\n",
    "    'Part_of': '',\n",
    "}\n",
    "\n",
    "for col, fill_value in key_columns_fillna.items():\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(fill_value)\n",
    "\n",
    "print(\"Filled missing values for key columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: ANALYZE NULL RATIOS\n",
    "# ============================================================================\n",
    "\n",
    "null_counts = df.isna().sum()\n",
    "null_ratios = null_counts / len(df)\n",
    "\n",
    "print(f\"\\nColumns with >50% missing values:\")\n",
    "high_null_cols = null_ratios[null_ratios > 0.5]\n",
    "print(high_null_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb3bbc0-886d-4bd6-bbce-0f0e44f0e8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Region\n",
       "South and Central Asia    532\n",
       "Oceania                   504\n",
       "North America             423\n",
       "Europe                    383\n",
       "East Asia                 358\n",
       "South America             253\n",
       "Africa                    235\n",
       "China                     214\n",
       "Russian Federation        147\n",
       "Middle East                63\n",
       "Antarctica                  2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5899681-6c64-471d-af84-9a0bb80ad814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_leakage(df, target_col='has_REE'):\n",
    "    \"\"\"\n",
    "    Comprehensive check for data leakage in features.\n",
    "    \n",
    "    This function checks:\n",
    "    1. Text fields that mention REE-related terms\n",
    "    2. Correlation between text mentions and target\n",
    "    3. Feature importance for potential leakage\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA LEAKAGE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # REE-related keywords to search for\n",
    "    ree_keywords = [\n",
    "        r'\\bREE\\b', r'\\bREEs\\b', r'\\bREO\\b',\n",
    "        r'rare earth', r'rare-earth',\n",
    "        r'lanthanide', r'lanthanoid',\n",
    "        r'cerium', r'lanthanum', r'neodymium', r'praseodymium',\n",
    "        r'samarium', r'europium', r'gadolinium', r'terbium',\n",
    "        r'dysprosium', r'holmium', r'erbium', r'thulium',\n",
    "        r'ytterbium', r'lutetium', r'yttrium', r'scandium',\n",
    "        r'bastnasite', r'bastnäsite', r'monazite', r'xenotime',\n",
    "        r'loparite', r'eudialyte', r'allanite', r'apatite.*REE'\n",
    "    ]\n",
    "    \n",
    "    ree_pattern = '|'.join(ree_keywords)\n",
    "    \n",
    "    # Text columns to check\n",
    "    text_cols = ['Dep_Type', 'Dep_Note', 'Rec_Note', 'Commods', \n",
    "                 'Status', 'Stat_Note', 'P_Status', 'Dep_Form']\n",
    "    \n",
    "    leakage_report = []\n",
    "    \n",
    "    print(\"\\n1. CHECKING TEXT FIELDS FOR REE MENTIONS:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Check for REE mentions\n",
    "        ree_mentions = df[col].fillna('').str.contains(\n",
    "            ree_pattern, \n",
    "            case=False, \n",
    "            regex=True\n",
    "        )\n",
    "        \n",
    "        n_mentions = ree_mentions.sum()\n",
    "        \n",
    "        if n_mentions > 0:\n",
    "            # Calculate correlation with target\n",
    "            has_ree_when_mentioned = df.loc[ree_mentions, target_col].mean()\n",
    "            has_ree_when_not_mentioned = df.loc[~ree_mentions, target_col].mean()\n",
    "            \n",
    "            # Calculate lift (how much more likely to have REE when mentioned)\n",
    "            baseline = df[target_col].mean()\n",
    "            lift = has_ree_when_mentioned / baseline if baseline > 0 else 0\n",
    "            \n",
    "            leakage_report.append({\n",
    "                'column': col,\n",
    "                'mentions': n_mentions,\n",
    "                'percent': n_mentions / len(df) * 100,\n",
    "                'has_ree_when_mentioned': has_ree_when_mentioned,\n",
    "                'has_ree_when_not': has_ree_when_not_mentioned,\n",
    "                'lift': lift\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Mentions REE: {n_mentions} rows ({n_mentions/len(df)*100:.1f}%)\")\n",
    "            print(f\"  has_REE when mentioned: {has_ree_when_mentioned:.1%}\")\n",
    "            print(f\"  has_REE when NOT mentioned: {has_ree_when_not_mentioned:.1%}\")\n",
    "            print(f\"  Lift: {lift:.2f}x\")\n",
    "            \n",
    "            if lift > 1.5:\n",
    "                print(f\"  ⚠️  WARNING: Strong leakage signal (lift > 1.5x)\")\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    if leakage_report:\n",
    "        leakage_df = pd.DataFrame(leakage_report)\n",
    "        leakage_df = leakage_df.sort_values('lift', ascending=False)\n",
    "        \n",
    "        print(\"\\n2. LEAKAGE SUMMARY (sorted by lift):\")\n",
    "        print(\"-\" * 70)\n",
    "        print(leakage_df.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n3. RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 70)\n",
    "        high_leakage = leakage_df[leakage_df['lift'] > 1.5]\n",
    "        if len(high_leakage) > 0:\n",
    "            print(\"⚠️  HIGH RISK columns (lift > 1.5x):\")\n",
    "            for col in high_leakage['column']:\n",
    "                print(f\"  - {col}: Should NOT be used for classification\")\n",
    "        \n",
    "        moderate_leakage = leakage_df[(leakage_df['lift'] > 1.2) & (leakage_df['lift'] <= 1.5)]\n",
    "        if len(moderate_leakage) > 0:\n",
    "            print(\"\\n⚠️  MODERATE RISK columns (lift 1.2-1.5x):\")\n",
    "            for col in moderate_leakage['column']:\n",
    "                print(f\"  - {col}: Use with caution, consider removing\")\n",
    "        \n",
    "        low_leakage = leakage_df[leakage_df['lift'] <= 1.2]\n",
    "        if len(low_leakage) > 0:\n",
    "            print(\"\\n✓ LOW RISK columns (lift <= 1.2x):\")\n",
    "            for col in low_leakage['column']:\n",
    "                print(f\"  - {col}: Probably safe to use\")\n",
    "    else:\n",
    "        print(\"\\n✓ No obvious REE mentions found in text fields\")\n",
    "    \n",
    "    return leakage_df if leakage_report else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6608ddff-2a4c-4a67-822c-92e509963187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_test_connectivity(edge_index, train_idx, test_idx):\n",
    "    \"\"\"\n",
    "    Check how many edges connect train and test nodes.\n",
    "    High connectivity can cause information leakage through the graph.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAIN-TEST GRAPH CONNECTIVITY CHECK\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    train_set = set(train_idx)\n",
    "    test_set = set(test_idx)\n",
    "    \n",
    "    cross_edges = 0\n",
    "    train_edges = 0\n",
    "    test_edges = 0\n",
    "    \n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src, dst = edge_index[0, i], edge_index[1, i]\n",
    "        \n",
    "        if src in train_set and dst in train_set:\n",
    "            train_edges += 1\n",
    "        elif src in test_set and dst in test_set:\n",
    "            test_edges += 1\n",
    "        elif (src in train_set and dst in test_set) or (src in test_set and dst in train_set):\n",
    "            cross_edges += 1\n",
    "    \n",
    "    total_edges = edge_index.shape[1]\n",
    "    \n",
    "    print(f\"\\nEdge distribution:\")\n",
    "    print(f\"  Train-Train edges: {train_edges} ({train_edges/total_edges*100:.1f}%)\")\n",
    "    print(f\"  Test-Test edges: {test_edges} ({test_edges/total_edges*100:.1f}%)\")\n",
    "    print(f\"  Train-Test edges: {cross_edges} ({cross_edges/total_edges*100:.1f}%)\")\n",
    "    \n",
    "    if cross_edges / total_edges > 0.2:\n",
    "        print(f\"\\n⚠️  WARNING: {cross_edges/total_edges*100:.1f}% of edges cross train-test boundary\")\n",
    "        print(\"  This can cause information leakage through the graph structure.\")\n",
    "        print(\"  Consider: reducing k, using different test region, or using inductive GNN\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Reasonable train-test separation ({cross_edges/total_edges*100:.1f}% cross-edges)\")\n",
    "    \n",
    "    return {\n",
    "        'train_edges': train_edges,\n",
    "        'test_edges': test_edges,\n",
    "        'cross_edges': cross_edges,\n",
    "        'cross_edge_ratio': cross_edges / total_edges\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc5677d-339e-4844-9c8c-d54321546053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING (LEAKAGE-FREE)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DATA LEAKAGE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. CHECKING TEXT FIELDS FOR REE MENTIONS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Dep_Note:\n",
      "  Mentions REE: 104 rows (3.3%)\n",
      "  has_REE when mentioned: 51.0%\n",
      "  has_REE when NOT mentioned: 65.5%\n",
      "  Lift: 0.78x\n",
      "\n",
      "Rec_Note:\n",
      "  Mentions REE: 28 rows (0.9%)\n",
      "  has_REE when mentioned: 71.4%\n",
      "  has_REE when NOT mentioned: 65.0%\n",
      "  Lift: 1.10x\n",
      "\n",
      "Commods:\n",
      "  Mentions REE: 3109 rows (99.8%)\n",
      "  has_REE when mentioned: 65.0%\n",
      "  has_REE when NOT mentioned: 80.0%\n",
      "  Lift: 1.00x\n",
      "\n",
      "2. LEAKAGE SUMMARY (sorted by lift):\n",
      "----------------------------------------------------------------------\n",
      "  column  mentions   percent  has_ree_when_mentioned  has_ree_when_not     lift\n",
      "Rec_Note        28  0.899165                0.714286          0.650032 1.097871\n",
      " Commods      3109 99.839435                0.650370          0.800000 0.999631\n",
      "Dep_Note       104  3.339756                0.509615          0.655482 0.783288\n",
      "\n",
      "3. RECOMMENDATIONS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "✓ LOW RISK columns (lift <= 1.2x):\n",
      "  - Rec_Note: Probably safe to use\n",
      "  - Commods: Probably safe to use\n",
      "  - Dep_Note: Probably safe to use\n",
      "\n",
      "======================================================================\n",
      "CREATING SAFE FEATURES\n",
      "======================================================================\n",
      "✓ Created standardized spatial features (lat_z, lon_z)\n",
      "✓ Created regional features (11 regions)\n",
      "✓ Created mineral system hierarchy features\n",
      "✓ Created record type features (3 types)\n",
      "✓ Created deposit classification features (REE-sanitized) (9 classes)\n",
      "⚠️  Skipping Dep_Form features (potential leakage risk)\n",
      "✓ Created commodity association features (REE-sanitized)\n",
      "⚠️  Skipping development status features (temporal leakage risk)\n",
      "\n",
      "======================================================================\n",
      "FEATURE MATRIX CREATED (LEAKAGE-FREE)\n",
      "======================================================================\n",
      "Total features: 26\n",
      "Total samples: 3114\n",
      "Missing values: 0\n",
      "\n",
      "Feature breakdown:\n",
      "  - Base features: 12\n",
      "  - Regional features: 11\n",
      "  - Record type features: 3\n",
      "  - System class features: 9\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: CLEAN FEATURE ENGINEERING (NO LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING (LEAKAGE-FREE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run leakage check first\n",
    "leakage_report = check_data_leakage(df)\n",
    "\n",
    "# Based on the leakage check, we'll exclude risky features\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING SAFE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.1 Spatial Features (Standardized) - SAFE\n",
    "# ---------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df[['lat_z', 'lon_z']] = scaler.fit_transform(df[['Latitude', 'Longitude']])\n",
    "print(\"✓ Created standardized spatial features (lat_z, lon_z)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.2 Regional Features (One-Hot Encoding) - SAFE\n",
    "# ---------------------------------------------------------------------------\n",
    "region_dummies = pd.get_dummies(df['Region'], prefix='region', drop_first=False)\n",
    "df = pd.concat([df, region_dummies], axis=1)\n",
    "print(f\"✓ Created regional features ({len(region_dummies.columns)} regions)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.3 Mineral System Hierarchy - SAFE\n",
    "# ---------------------------------------------------------------------------\n",
    "df['is_composite_system'] = df['Components'].notna().astype(int)\n",
    "df['is_part_of_complex'] = df['Part_of'].notna().astype(int)\n",
    "print(\"✓ Created mineral system hierarchy features\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.4 Record Type Features - SAFE\n",
    "# ---------------------------------------------------------------------------\n",
    "# rec_type_dummies = pd.get_dummies(df['Rec_Type_clean'], prefix='rec_type', drop_first=False)\n",
    "rec_type_dummies = pd.get_dummies(df['Rec_Type'], prefix='rec_type', drop_first=False)\n",
    "df = pd.concat([df, rec_type_dummies], axis=1)\n",
    "print(f\"✓ Created record type features ({len(rec_type_dummies.columns)} types)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.5 Deposit Classification - RISKY (based on text)\n",
    "# ---------------------------------------------------------------------------\n",
    "# Option A: Use with cleaned text (remove REE mentions)\n",
    "# Option B: Skip entirely and rely on other features\n",
    "# Let's use Option A with cleaned text\n",
    "\n",
    "def classify_system(row):\n",
    "    \"\"\"\n",
    "    Classify deposit type WITHOUT using REE-specific information.\n",
    "    This removes any mention of REE-related terms before classification.\n",
    "    \"\"\"\n",
    "    # Get text fields\n",
    "    text = ' '.join([\n",
    "        str(row.get('Dep_Type', '')),\n",
    "        str(row.get('Dep_Note', '')),\n",
    "        str(row.get('Rec_Note', ''))\n",
    "    ])\n",
    "    \n",
    "    # Remove REE-related keywords to prevent leakage\n",
    "    ree_pattern = r'(REE|rare earth|lanthanide|monazite|bastn[aä]site|xenotime|loparite|eudialyte)'\n",
    "    text_clean = re.sub(ree_pattern, '', text, flags=re.IGNORECASE).lower()\n",
    "    \n",
    "    # Now classify based on geological characteristics only\n",
    "    if 'carbonatite' in text_clean:\n",
    "        return 'carbonatite'\n",
    "    if 'alkaline' in text_clean or 'alkali' in text_clean:\n",
    "        return 'alkaline_intrusive'\n",
    "    if 'placer' in text_clean:\n",
    "        return 'placer'\n",
    "    if any(k in text_clean for k in ['clay', 'laterite', 'ion adsorption', 'ion-adsorption']):\n",
    "        return 'clay_laterite'\n",
    "    if 'pegmatite' in text_clean:\n",
    "        return 'pegmatite'\n",
    "    if 'vein' in text_clean:\n",
    "        return 'vein'\n",
    "    if any(k in text_clean for k in ['skarn', 'contact']):\n",
    "        return 'skarn'\n",
    "    if 'hydrothermal' in text_clean:\n",
    "        return 'hydrothermal'\n",
    "    if str(row.get('Dep_Type', '')).strip().lower() not in ['', 'nan', 'unknown']:\n",
    "        return 'other'\n",
    "    return 'unknown'\n",
    "\n",
    "def classify_system_prediscovery(row):\n",
    "    \"\"\"\n",
    "    Classify based ONLY on host rock type, not mineralization.\n",
    "    This should be knowable before any REE analysis.\n",
    "    \"\"\"\n",
    "    dep_type = str(row.get('Dep_Type', '')).lower()\n",
    "    \n",
    "    # Use only host rock terms, not mineral names\n",
    "    if 'carbonatite' in dep_type:\n",
    "        return 'carbonatite_host'\n",
    "    if any(x in dep_type for x in ['alkaline', 'alkali', 'syenite', 'nepheline']):\n",
    "        return 'alkaline_host'\n",
    "    if any(x in dep_type for x in ['granite', 'pegmatite']):\n",
    "        return 'felsic_host'\n",
    "    if any(x in dep_type for x in ['sediment', 'sandstone', 'conglomerate']):\n",
    "        return 'sedimentary_host'\n",
    "    if 'metamorphic' in dep_type:\n",
    "        return 'metamorphic_host'\n",
    "    \n",
    "    return 'unknown_host'\n",
    "\n",
    "df['system_class'] = df.apply(classify_system, axis=1)\n",
    "system_class_dummies = pd.get_dummies(df['system_class'], prefix='system', drop_first=False)\n",
    "df = pd.concat([df, system_class_dummies], axis=1)\n",
    "print(f\"✓ Created deposit classification features (REE-sanitized) ({len(system_class_dummies.columns)} classes)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.6 Deposit Form Features - POTENTIALLY RISKY\n",
    "# ---------------------------------------------------------------------------\n",
    "# Skip this for now as it might contain REE-specific information\n",
    "print(\"⚠️  Skipping Dep_Form features (potential leakage risk)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.7 Commodity Association Features - SAFE (excluding REE)\n",
    "# ---------------------------------------------------------------------------\n",
    "def has_element_safe(series, element):\n",
    "    \"\"\"Check if element is present, but exclude rows that mention REE.\"\"\"\n",
    "    # First check for the element\n",
    "    has_elem = series.str.contains(element, case=False, na=False)\n",
    "    \n",
    "    # Exclude if REE is also mentioned (to avoid correlation through REE deposits)\n",
    "    mentions_ree = series.str.contains(r'\\bREE\\b|rare earth', case=False, na=False)\n",
    "    \n",
    "    # Only count as having element if REE is NOT mentioned\n",
    "    return (has_elem & ~mentions_ree).astype(int)\n",
    "\n",
    "# Use safe version that excludes REE-mentioning commodities\n",
    "df['has_Nb'] = has_element_safe(df['Commods'], r'\\bNb\\b')\n",
    "df['has_Ta'] = has_element_safe(df['Commods'], r'\\bTa\\b')\n",
    "df['has_Th'] = has_element_safe(df['Commods'], r'\\bTh\\b')\n",
    "df['has_U'] = has_element_safe(df['Commods'], r'\\bU\\b')\n",
    "df['has_P'] = has_element_safe(df['Commods'], r'\\bP\\b')\n",
    "df['has_F'] = has_element_safe(df['Commods'], r'\\bF\\b')\n",
    "df['has_Zr'] = has_element_safe(df['Commods'], r'\\bZr\\b')\n",
    "\n",
    "# Count of associated commodities\n",
    "df['commodity_count'] = (\n",
    "    df[['has_Nb', 'has_Ta', 'has_Th', 'has_U', 'has_P', 'has_F', 'has_Zr']].sum(axis=1)\n",
    ")\n",
    "print(\"✓ Created commodity association features (REE-sanitized)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.8 Development Status Features - RISKY (known after discovery)\n",
    "# ---------------------------------------------------------------------------\n",
    "# These are known AFTER a deposit is characterized, so they can leak information\n",
    "# SKIP these features\n",
    "print(\"⚠️  Skipping development status features (temporal leakage risk)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SELECT FEATURES FOR MODEL (CONSERVATIVE)\n",
    "# ============================================================================\n",
    "\n",
    "# Base features (safe)\n",
    "base_features = [\n",
    "    'lat_z', 'lon_z',\n",
    "    'is_composite_system', 'is_part_of_complex',\n",
    "    'has_Nb', 'has_Ta', 'has_Th', 'has_U', 'has_P', 'has_F', 'has_Zr',\n",
    "    'commodity_count'\n",
    "]\n",
    "\n",
    "# Categorical features (safe)\n",
    "categorical_features = (\n",
    "    list(region_dummies.columns) +\n",
    "    list(rec_type_dummies.columns) \n",
    ")\n",
    "\n",
    "# +\n",
    "#     list(system_class_dummies.columns)\n",
    "\n",
    "# Combine all features\n",
    "gnn_features = base_features + categorical_features\n",
    "# gnn_features = base_features\n",
    "\n",
    "# Create feature matrix\n",
    "X = df[gnn_features].astype(float)\n",
    "y = df['has_REE'].astype(int)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"FEATURE MATRIX CREATED (LEAKAGE-FREE)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Total samples: {X.shape[0]}\")\n",
    "print(f\"Missing values: {X.isna().sum().sum()}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Base features: {len(base_features)}\")\n",
    "print(f\"  - Regional features: {len(region_dummies.columns)}\")\n",
    "print(f\"  - Record type features: {len(rec_type_dummies.columns)}\")\n",
    "print(f\"  - System class features: {len(system_class_dummies.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8acf5ca2-968d-49b6-9a41-64a66ba4b6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['region_Africa', 'region_Antarctica', 'region_China',\n",
       "       'region_East Asia', 'region_Europe', 'region_Middle East',\n",
       "       'region_North America', 'region_Oceania', 'region_Russian Federation',\n",
       "       'region_South America', 'region_South and Central Asia'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2022cd37-9e9d-4db1-bd68-e5a08b7d5fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rec_type_district or area', 'rec_type_intrusion or complex',\n",
       "       'rec_type_site'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_type_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e311cb-99cc-4139-a6c4-ef6f04d67703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['system_alkaline_intrusive', 'system_carbonatite',\n",
       "       'system_clay_laterite', 'system_hydrothermal', 'system_other',\n",
       "       'system_pegmatite', 'system_placer', 'system_skarn', 'system_vein'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_class_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22879ab3-b396-429a-a6d3-902046954657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3114, 26)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e921a16e-eb0e-452a-b295-fe68f95d0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BUILDING GRAPH STRUCTURE\n",
      "======================================================================\n",
      "✓ Created k-NN graph (k=10)\n",
      "  - Total edges: 62280\n",
      "  - Average degree: 20.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: BUILD GRAPH STRUCTURE (k-NN SPATIAL EDGES)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BUILDING GRAPH STRUCTURE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "coords = df[['Latitude', 'Longitude']].values\n",
    "\n",
    "# Build k-nearest neighbors graph\n",
    "k = 10  # Number of nearest neighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric='haversine')\n",
    "nbrs.fit(np.radians(coords))\n",
    "\n",
    "distances, indices = nbrs.kneighbors(np.radians(coords))\n",
    "\n",
    "# Create edge list\n",
    "edge_list = []\n",
    "for i in range(indices.shape[0]):\n",
    "    for j in indices[i][1:]:  # Skip self-loop\n",
    "        edge_list.append((i, j))\n",
    "\n",
    "# Convert to edge index (remove duplicates and make bidirectional)\n",
    "edge_index = np.array(list(set(edge_list))).T\n",
    "edge_index = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "\n",
    "print(f\"✓ Created k-NN graph (k={k})\")\n",
    "print(f\"  - Total edges: {edge_index.shape[1]}\")\n",
    "print(f\"  - Average degree: {edge_index.shape[1] / len(df):.1f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af26a21-3eb4-45a1-935b-30c89bb61a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: TRAIN/TEST SPLIT FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_spatial_split(df, X, y, edge_index, test_region, verbose=True):\n",
    "    \"\"\"\n",
    "    Create train/test split based on spatial holdout (by region).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Original dataframe with 'Region' column\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "    y : pd.Series\n",
    "        Target labels\n",
    "    edge_index : np.ndarray\n",
    "        Graph edge indices\n",
    "    test_region : str\n",
    "        Region to hold out for testing\n",
    "    verbose : bool\n",
    "        Whether to print statistics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing:\n",
    "        - X_train, X_test: Feature matrices for train/test\n",
    "        - y_train, y_test: Labels for train/test\n",
    "        - train_idx, test_idx: Node indices for train/test\n",
    "        - train_mask, test_mask: PyTorch boolean masks\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"TRAIN/TEST SPLIT (SPATIAL HOLDOUT)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        regions = df['Region'].unique()\n",
    "        print(f\"\\nAvailable regions: {list(regions)}\")\n",
    "        \n",
    "        if test_region not in regions:\n",
    "            raise ValueError(f\"Test region '{test_region}' not found in data. Available: {list(regions)}\")\n",
    "    \n",
    "    # Create boolean masks\n",
    "    test_idx_bool = df['Region'] == test_region\n",
    "    train_idx_bool = ~test_idx_bool\n",
    "    \n",
    "    # Get integer indices\n",
    "    train_idx = np.where(train_idx_bool)[0]\n",
    "    test_idx = np.where(test_idx_bool)[0]\n",
    "    \n",
    "    # Create train/test splits\n",
    "    X_train = X.iloc[train_idx] if hasattr(X, 'iloc') else X[train_idx]\n",
    "    X_test = X.iloc[test_idx] if hasattr(X, 'iloc') else X[test_idx]\n",
    "    y_train = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]\n",
    "    y_test = y.iloc[test_idx] if hasattr(y, 'iloc') else y[test_idx]\n",
    "    \n",
    "    # Create PyTorch masks for GNN\n",
    "    num_nodes = len(df)\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTest region: {test_region}\")\n",
    "        print(f\"Train nodes: {len(train_idx)} ({len(train_idx)/len(df):.1%})\")\n",
    "        print(f\"Test nodes: {len(test_idx)} ({len(test_idx)/len(df):.1%})\")\n",
    "        print(f\"\\nClass distribution:\")\n",
    "        print(f\"  Train - has_REE: {y_train.mean():.2%}\")\n",
    "        print(f\"  Test  - has_REE: {y_test.mean():.2%}\")\n",
    "    \n",
    "    split_dict = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'train_idx': train_idx,\n",
    "        'test_idx': test_idx,\n",
    "        'train_mask': train_mask,\n",
    "        'test_mask': test_mask,\n",
    "    }\n",
    "    \n",
    "    # Check train-test connectivity\n",
    "    if verbose:\n",
    "        check_train_test_connectivity(edge_index, train_idx, test_idx)\n",
    "    \n",
    "    return split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e3dafa2-745b-4501-a36e-48bd543bc666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAIN/TEST SPLIT (SPATIAL HOLDOUT)\n",
      "======================================================================\n",
      "\n",
      "Available regions: ['Africa', 'Antarctica', 'China', 'East Asia', 'Europe', 'Middle East', 'North America', 'Oceania', 'Russian Federation', 'South America', 'South and Central Asia']\n",
      "\n",
      "Test region: China\n",
      "Train nodes: 2900 (93.1%)\n",
      "Test nodes: 214 (6.9%)\n",
      "\n",
      "Class distribution:\n",
      "  Train - has_REE: 66.38%\n",
      "  Test  - has_REE: 47.20%\n",
      "\n",
      "======================================================================\n",
      "TRAIN-TEST GRAPH CONNECTIVITY CHECK\n",
      "======================================================================\n",
      "\n",
      "Edge distribution:\n",
      "  Train-Train edges: 57792 (92.8%)\n",
      "  Test-Test edges: 3708 (6.0%)\n",
      "  Train-Test edges: 780 (1.3%)\n",
      "\n",
      "✓ Reasonable train-test separation (1.3% cross-edges)\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "======================================================================\n",
      "MODEL CONFIGURATION\n",
      "======================================================================\n",
      "Architecture: GCN\n",
      "Hidden channels: 64\n",
      "Number of layers: 3\n",
      "Dropout: 0.5\n",
      "Learning rate: 0.01\n",
      "Weight decay: 0.0005\n",
      "Max epochs: 200\n",
      "Early stopping patience: 20\n",
      "Preparing data for GNN...\n",
      "✓ Created PyG Data object\n",
      "  - Nodes: 3114\n",
      "  - Edges: 62280\n",
      "  - Features: 26\n",
      "  - Train nodes: 2900\n",
      "  - Test nodes: 214\n",
      "\n",
      "✓ Initialized GCN model\n",
      "  Total parameters: 6274\n",
      "\n",
      "======================================================================\n",
      "TRAINING GNN MODEL\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|██████▌                                                          | 20/200 [00:00<00:01, 143.23it/s, Loss=0.5264, Train Acc=0.7566, Test F1=0.0917, Test AUC=0.5870]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 21\n",
      "\n",
      "======================================================================\n",
      "FINAL EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "TRAIN SET:\n",
      "  Accuracy:  0.7393\n",
      "  Precision: 0.8562\n",
      "  Recall:    0.7299\n",
      "  F1 Score:  0.7880\n",
      "  AUC-ROC:   0.8194\n",
      "\n",
      "TEST SET:\n",
      "  Accuracy:  0.5374\n",
      "  Precision: 0.6250\n",
      "  Recall:    0.0495\n",
      "  F1 Score:  0.0917\n",
      "  AUC-ROC:   0.5895\n",
      "\n",
      "CONFUSION MATRIX (Test Set):\n",
      "[[110   3]\n",
      " [ 96   5]]\n",
      "\n",
      "TN: 110, FP: 3\n",
      "FN: 96, TP: 5\n",
      "\n",
      "CLASSIFICATION REPORT (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No REE       0.53      0.97      0.69       113\n",
      "     Has REE       0.62      0.05      0.09       101\n",
      "\n",
      "    accuracy                           0.54       214\n",
      "   macro avg       0.58      0.51      0.39       214\n",
      "weighted avg       0.58      0.54      0.41       214\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved training history plot to training_history.png\n"
     ]
    }
   ],
   "source": [
    "from gnn_model import train_gnn_model\n",
    "test_region = 'China'  # Change this to test different regions\n",
    "split_data = create_spatial_split(df, X, y, edge_index, test_region)\n",
    "\n",
    "model, history, metrics = train_gnn_model(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        edge_index=edge_index,\n",
    "        split_data=split_data,\n",
    "        model_type='GCN',\n",
    "        hidden_channels=64,\n",
    "        num_layers=3,\n",
    "        dropout=0.5,\n",
    "        lr=0.01,\n",
    "        epochs=200,\n",
    "        patience=20\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23dcad8a-7b13-4be4-84ed-165ed28018c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "\n",
      "======================================================================\n",
      "MODEL CONFIGURATION\n",
      "======================================================================\n",
      "Architecture: GAT\n",
      "Hidden channels: 64\n",
      "Number of layers: 3\n",
      "Dropout: 0.5\n",
      "Learning rate: 0.01\n",
      "Weight decay: 0.0005\n",
      "Max epochs: 200\n",
      "Early stopping patience: 20\n",
      "Preparing data for GNN...\n",
      "✓ Created PyG Data object\n",
      "  - Nodes: 3114\n",
      "  - Edges: 62280\n",
      "  - Features: 26\n",
      "  - Train nodes: 2900\n",
      "  - Test nodes: 214\n",
      "\n",
      "✓ Initialized GAT model\n",
      "  Total parameters: 74246\n",
      "\n",
      "======================================================================\n",
      "TRAINING GNN MODEL\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|███████▌                                                          | 23/200 [00:00<00:03, 55.69it/s, Loss=0.6021, Train Acc=0.7341, Test F1=0.4183, Test AUC=0.5906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 24\n",
      "\n",
      "======================================================================\n",
      "FINAL EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "TRAIN SET:\n",
      "  Accuracy:  0.6710\n",
      "  Precision: 0.8332\n",
      "  Recall:    0.6306\n",
      "  F1 Score:  0.7179\n",
      "  AUC-ROC:   0.7702\n",
      "\n",
      "TEST SET:\n",
      "  Accuracy:  0.5374\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.0198\n",
      "  F1 Score:  0.0388\n",
      "  AUC-ROC:   0.5959\n",
      "\n",
      "CONFUSION MATRIX (Test Set):\n",
      "[[113   0]\n",
      " [ 99   2]]\n",
      "\n",
      "TN: 113, FP: 0\n",
      "FN: 99, TP: 2\n",
      "\n",
      "CLASSIFICATION REPORT (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No REE       0.53      1.00      0.70       113\n",
      "     Has REE       1.00      0.02      0.04       101\n",
      "\n",
      "    accuracy                           0.54       214\n",
      "   macro avg       0.77      0.51      0.37       214\n",
      "weighted avg       0.75      0.54      0.39       214\n",
      "\n",
      "\n",
      "✓ Saved training history plot to training_history.png\n"
     ]
    }
   ],
   "source": [
    "model, history, metrics = train_gnn_model(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        edge_index=edge_index,\n",
    "        split_data=split_data,\n",
    "        model_type='GAT',\n",
    "        hidden_channels=64,\n",
    "        num_layers=3,\n",
    "        dropout=0.5,\n",
    "        lr=0.01,\n",
    "        epochs=200,\n",
    "        patience=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dffbe7dc-fc3a-409d-972a-9e1ae9d9c425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REE prevalence by system_class:\n",
      "                        mean  count\n",
      "system_class                       \n",
      "placer              0.903970   1083\n",
      "pegmatite           0.712727    275\n",
      "hydrothermal        0.693878     49\n",
      "carbonatite         0.632075    424\n",
      "alkaline_intrusive  0.604905    367\n",
      "vein                0.567901     81\n",
      "skarn               0.474576     59\n",
      "clay_laterite       0.366197     71\n",
      "other               0.321986    705\n",
      "\n",
      "Overall baseline: 65.1%\n"
     ]
    }
   ],
   "source": [
    "# Check REE prevalence by system class\n",
    "print(\"REE prevalence by system_class:\")\n",
    "system_ree = df.groupby('system_class')['has_REE'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "print(system_ree)\n",
    "print(f\"\\nOverall baseline: {df['has_REE'].mean():.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
